
services:

# cache para uso geral
  redis:
    image: redis:alpine
    shm_size: 128mb
    user: "${UID}:${GID}"
    ports:
      - "6379:6379"
    volumes:
      - ./redis_volume:/data

# banco para uso geral e também como banco para o n8n
  postgres:
    image: postgres:alpine
    shm_size: 128mb
    user: "${UID}:${GID}"
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_DB
      - POSTGRES_NON_ROOT_USER
      - POSTGRES_NON_ROOT_PASSWORD
    ports:
      - "5432:5432"
    volumes:
      - ./postgres_volume:/var/lib/postgresql/data:z
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -h localhost -U ${POSTGRES_USER} -d ${POSTGRES_DB}']
      interval: 5s
      timeout: 5s
      retries: 10

  n8n:
    image: docker.n8n.io/n8nio/n8n
    user: "${UID}:${GID}"
    ports:
      - "5678:5678"
    volumes:
      - ./n8n_volume:/home/node/
    environment:
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=${POSTGRES_DB}
      - DB_POSTGRESDB_USER=${POSTGRES_NON_ROOT_USER}
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_NON_ROOT_PASSWORD}
    depends_on:
      postgres:
        condition: service_healthy

# ferramenta para integração com o Whatsapp
  waha:
    image: devlikeapro/waha:noweb
    user: "${UID}:${GID}"
    volumes:
      - ./waha_volume:/app/.sessions
    ports:
      - "3000:3000"

# ferramenta para rodar localmente as AI LLM's
  ollama:
    image: ollama/ollama
    runtime: nvidia # para poder usar a GPU da nVidia, não possuo GPU amd para validar e configurar =´(
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./ollama_volume:/root/.ollama
      - ./ollama_models_volume:/models
    ports:
      - "11434:11434"

# interface grafica web para usar o ollama diretamente.
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    environment:
      - MODEL_DOWNLOAD_DIR=/models
      - OLLAMA_API_BASE_URL=http://ollama:11434
      - OLLAMA_API_URL=http://ollama:11434
      - OLLAMA_BASE_URL=http://ollama:11434 # https://github.com/open-webui/open-webui/blob/b5f4c85bb196c16a775802907aedd87366f58b0f/backend/open_webui/config.py#L820
      - LOG_LEVEL=debug
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY} 
    ports:
      - "8080:8080"
    depends_on:
      - ollama